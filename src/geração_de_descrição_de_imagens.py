# -*- coding: utf-8 -*-
"""Geração de Descrição de imagens.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fCPORsDnrLU5iYaKbQKFuBZUlbXifuh_
"""

#https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/
#https://huggingface.co/baseplate/vit-gpt2-image-captioning

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers
!pip install deep_translator
!pip install pyttsx3
!sudo apt install espeak

import numpy as np
import pandas as pd
from os import path
from PIL import Image
import cv2
import os
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import glob, os

from transformers import VisionEncoderDecoderModel
from transformers import AutoTokenizer
from transformers import ViTFeatureExtractor
from transformers import ViTImageProcessor, BertTokenizer, VisionEncoderDecoderModel

import torch
import torchvision.transforms as T
import torchvision
from torch.autograd import Variable
from PIL import Image

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from transformers import GPT2Config

import time
from deep_translator import GoogleTranslator

model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
feature_extractor = ViTFeatureExtractor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

tokenizer.pad_token = tokenizer.eos_token

dirbase="/content/drive/MyDrive/IA-Semestre1/Projeto-IA/train"
folder = dirbase
pasta_dataset = os.chdir(folder)
i = 0

learning_rate = 1e-4
num_epochs = 200
num_classes=10000

train_text_descriptions = [
"a light painting of a crowd of people on top of a raft in a dark blue ocean"
,"a light painting of a crowd of people standing in a front of a statue praying"
,"a dark painting of a supper with some food and some people around the table"
,"a light painting of trees in a light blue sky"
,"a light painting of a crownd of people dancing with music"
,"a light painting of a gourd in a chest"
,"a dark painting of starving animals in a desert"
,"a light painting of a crowd of people and animals manufacturing"
,"a light painting of a sad man with a beard and a mustache"
,"a light painting of a crowd of people wornking on a coffee plantation"
,"a dark painting of a fat man with a brown coat and a cross necklace"
,"a light painting of a crowd of people and a horse on a forest"
,"a light painting of an old fashioned picture of a farm house"
,"a dark painting of a crowd of people with a clown in a dirt field"
,"a light painting of a crowd of people on a coffee plantation"
,"a light painting of a light blue seashell mural"
,"a dark painting of oil deposit tanks on the distance in a hill"
,"a dark painting of a crowd of people on a field dancing"
,"a dark painting of crowd people standing beside a dead body"
,"a light painting of a person sitting on a stage with a crowd of people around"
,"a light painting of wheat field with a scarecrow"
,"a light painting of a scarecrow in a field with ballons behind"
,"a light painting of a scarecrow in a field with big linearts"
,"a dark painting of a vase filled with flowers on top of a table"
,"a dark painting of a group of animals in a dark forest"
,"a light painting of a crowd of people wornking on a coffee plantation"
,"a dark painting of a chicken with two heads in dark blue sky"
,"a light painting of crowd of people in a river"
,"a light painting of a collage of photos of people"
,"a dark painting of two naked persons beside a rock"
,"a light painting of line-art woman in a raft"
,"a light painting of sailboat in the middle of the ocean"
,"a large rock with a couple of sail boats in the distance"
,"a light painting of a beach of dark brown rocks near the ocean"
,"a light painting of a person flying kites on the beach"
,"a dark painting of a iron tools on a workbench"
,"a dark painting of a woman an d a child in the beach"
,"a dark painting of a bottle of alcohol on a table next to a painting"
,"a light painting of a bottle of alcohol on a table next to a painting"
,"a dark painting of a bottle of alcohol on a table next to fruits"
,"a dark painting of a wooden boat with a group of people on the deck"
,"a light painting of a naked woman sitting on a chair"
,"a light painting of a person in a farm with a farmtool"
,"a dark painting of a group of starving people on a train track"
,"a light painting of beach with trees on top of a lush green field"
,"a dark painting of a cow standing on top of a lush green field"
]

dataset = []

for style in sorted(glob.glob("*")):
    path = folder+"/"+style
    image_dataset = {
        'image': path,
        'caption': train_text_descriptions[i]
    }
    i = i + 1
    dataset.append(image_dataset)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

config = GPT2Config(hidden_size=1024)
hidden_size = config.hidden_size

# Add your custom head or classifier on top of the model
classifier = nn.Linear(hidden_size, num_classes)
classifier.to(device)

# # Step 5: Freeze or unfreeze model layers (optional)
# # By default, the model layers are unfrozen and will be fine-tuned during training

# # Step 6: Define your loss function
loss_fn = nn.CrossEntropyLoss()

# Step 7: Train the model
optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)

# Create a data loader for your preprocessed dataset

transform = torchvision.transforms.ToTensor()

# Training loop
model.train()
for epoch in range(num_epochs):
    for data in dataset:
        # Move batch to device
        open_image = Image.open(data['image'])
        tensor_image = transform(open_image)
        encoded_inputs = tokenizer.encode_plus(data['caption'], padding="max_length", max_length=128, truncation=True, return_tensors="pt")
        tensor_caption = encoded_inputs
        tensor_image.to(device)
        tensor_caption.to(device)
        # Forward pass
        features = feature_extractor(open_image, return_tensors="pt").pixel_values
        features = features.to(device)
        outputs = model(pixel_values=features, decoder_input_ids=tensor_caption['input_ids'])
        logits = outputs.logits

        # Compute the loss
        loss = loss_fn(logits.view(-1, logits.shape[-1]), tensor_caption['input_ids'].view(-1))

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()



max_length = 22
num_beams = 4
top_k=50,
top_p=0.95,
num_return_sequences=3
use_cache = True

gen_kwargs = {"max_length": max_length, "num_beams": num_beams, "top_k": top_k, "num_return_sequences": num_return_sequences, "top_p": top_p, "use_cache": use_cache  }

def predict_step(image_paths):
  images = []
  for image_path in image_paths:
    i_image = Image.open(image_path)
    if i_image.mode != "RGB":
      i_image = i_image.convert(mode="RGB")

    images.append(i_image)

  pixel_values = feature_extractor(images=images, return_tensors="pt").pixel_values
  pixel_values = pixel_values.to(device)

  output_ids = model.generate(pixel_values, **gen_kwargs)

  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
  preds = [pred.strip() for pred in preds]

  return preds

def ShowImage(path):
  image = mpimg.imread(path)
  plt.imshow(image)
  plt.title(predict_step([path])[0]) # aqui ele devolve a string de descrição
  plt.axis("off")
  plt.show()

def ShowString(path):
  return predict_step([path])[0] # aqui ele devolve a string de descrição

def CropImage(path,dest):

    img_name = (path.split("/")[len(path.split("/"))-1]).split(".")[0]
    print(img_name)

    img = cv2.imread(path)

    width = img.shape[1]
    height = img.shape[0]

    print(width,height)
    half = (int(height/2),int(width/2))

    img_q1 = img[0:half[0], 0:half[1]]
    img_q2 = img[half[0]:height, 0:half[1]]
    img_q3 = img[half[0]:height, half[1]:width]
    img_q4 = img[0:half[0],half[1]:width]


    img_top    = img[0:half[0], 0:width]
    img_botton = img[half[0]:height, 0:width]
    img_right  = img[0:height, 0:half[1]]
    img_left   = img[0:height, half[1]:width]


    print(dest+img_name)
    cv2.imwrite(dest+img_name+"_q1.jpg",img_q1)
    cv2.imwrite(dest+img_name+"_q2.jpg",img_q2)
    cv2.imwrite(dest+img_name+"_q3.jpg",img_q3)
    cv2.imwrite(dest+img_name+"_q4.jpg",img_q4)

    cv2.imwrite(dest+img_name+"_top.jpg",img_top)
    cv2.imwrite(dest+img_name+"_botton.jpg",img_botton)
    cv2.imwrite(dest+img_name+"_right.jpg",img_right)
    cv2.imwrite(dest+img_name+"_left.jpg",img_left)

    return str(dest+img_name)

def Captioning(path,dest):
    list_captions = []
    path_quadrantes = CropImage(path,dest)

    ShowImage(path)
    list_captions.append(ShowString(path))
    os.chdir(dest)

    for cropped_img in glob.glob("*"):
        print(cropped_img)

        ShowImage(cropped_img)

        list_captions.append(ShowString(cropped_img))

        if os.path.exists(cropped_img):
            os.remove(cropped_img)
        else:
            print("The file does not exist")
    return list_captions

def plot(imgs, with_orig=True, row_title=None, **imshow_kwargs):
    if not isinstance(imgs[0], list):
        # Make a 2d grid even if there's just 1 row
        imgs = [imgs]

    num_rows = len(imgs)
    num_cols = len(imgs[0]) + with_orig
    fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False,  figsize=(20, 15))
    for row_idx, row in enumerate(imgs):
        row = [orig_img] + row if with_orig else row
        for col_idx, img in enumerate(row):
            ax = axs[row_idx, col_idx]
            ax.imshow(np.asarray(img), **imshow_kwargs)
            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])

    if row_title is not None:
        for row_idx in range(num_rows):
            axs[row_idx, 0].set(ylabel=row_title[row_idx])

    plt.tight_layout()

def ImageCaptioning(imgset):
    pixel_values = feature_extractor(images=imgset, return_tensors="pt").pixel_values

    pixel_values = pixel_values.to(device)

    output_ids = model.generate(pixel_values, **gen_kwargs)

    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
    preds = [pred.strip() for pred in preds]

    torch.cuda.empty_cache()
    return preds

def Image_Cropping(orig_img):
    width  = orig_img.size[0]
    height = orig_img.size[1]
    (top_left, top_right, bottom_left, bottom_right, center) = T.FiveCrop(size=(height/2,width/2))(orig_img)
    #plot([top_left, top_right, bottom_left, bottom_right, center])
    imgset = [orig_img,top_left, top_right, bottom_left, bottom_right, center]

    return imgset

#sudo apt install espeak
import pyttsx3

engine = pyttsx3.init() # object creation

def Text_to_Speech_Setup(engine):
    """ RATE"""
    rate = engine.getProperty('rate')   # getting details of current speaking rate
    #print (rate)                        #printing current voice rate
    engine.setProperty('rate', 150)     # setting up new voice rate


    """VOLUME"""
    volume = engine.getProperty('volume')   #getting to know current volume level (min=0 and max=1)
    #print (volume)                          #printing current volume level
    engine.setProperty('volume',1.0)    # setting up volume level  between 0 and 1

    """VOICE"""
    voices = engine.getProperty('voices')
    #engine.setProperty('voice', "english-us")
    engine.setProperty("voice", "brazil")

import time
from deep_translator import GoogleTranslator

def Run_Text_to_Speech(preds):
    final_sentence = []
    spatial_coord = ["............. ......... The summary of the scene is ",   "............. ......... In the upper left corner...", "............. ......... In the upper right corner...", "............. ......... In the lower left corner...",  "............. ......... In the lower right corner...","............. ......... In the center of the scene..."]


    for line in range(len(preds)):
        PrintMask(line,orig_img)
     #   time.sleep(5)
        final_sentence.append(spatial_coord[line]+preds[line])
    #    time.sleep(5)
        translated = GoogleTranslator(source='en', target='pt').translate(final_sentence[line])
        print(translated)
        #engine.say(translated)
        #engine.runAndWait()
        #engine.stop()

#half opacity
from PIL import Image
from PIL import ImageDraw

def PrintMask(part,orig_img):
    #image=Image.open('./workplace.jpg')
    copy_img = orig_img.copy()
    poly = Image.new('RGBA', orig_img.size)

    #print(poly.size)
    pdraw = ImageDraw.Draw(poly)

    w, h = orig_img.size[0], orig_img.size[1]

    shape = [ (0, 0), (w,h) ]

    if(part==0):
        shape = [ (0, 0), (w,h) ]
    if(part==1):
        shape = [ (0, 0), ( int(w/2) , int(h/2) ) ]
    if(part==2):
        shape = [ ( int(w/2),0 ), ( w,int(h/2)) ]
    if(part==3):
        shape = [ ( 0 , int(h/2) ), (int(w/2),h) ]
    if(part==4):
        shape = [ ( int(w/2) , int(h/2) ), (w,h)  ]
    if(part==5):
        shape = [ ( int(w/2)-int(w/4) , int(h/2)-int(h/4) ), (int(w/2)+int(w/4) , int(h/2)+int(h/4))  ]

    pdraw.rectangle(shape, fill=(255,0,255,50),outline ="red", width=10)

    copy_img.paste(poly,mask=poly)

    plt.figure(figsize=(3,3))
    plt.imshow(copy_img)
    plt.show(block=True)

class ArvoreOntologica:
    def __init__(self, nome):
        self.nome = nome
        self.nos = []

    def adiciona_no(self, no_filho):
        self.nos.append(no_filho)

    def busca_palavra(self, palavra):
        if self.nome == palavra:
            return True
        for no in self.nos:
            if no.busca_palavra(palavra):
                return True
        return False

def popular_arvore_antologica(word_cloud_data):
    wordcloud = WordCloud().generate(word_cloud_data)

    termos_relevante = []
    for word, freq in wordcloud.words_.items():
      if(freq>=0.25):
          termos_relevante.append(word)

    nos_ontologicos = []
    for term in termos_relevante:
        no = ArvoreOntologica(term)
        nos_ontologicos.append(no)

    root = ArvoreOntologica("Thing")
    for no in nos_ontologicos:
        root.adiciona_no(no)

    return root

word_cloud_data = "painting painting woman black white white photo standing woman sitting painting group group people painting person beach animal collage photos flying sitting sitting table flower horse statue sitting wooden front painting field table water sitting bench graffiti bench painting standing front photo black wooden table photos people building giraffe laying ground stone people standing ground person filled flowers skateboard surfboard hydrant riding horse woman holding middle bottle sitting front person laying piece group animals covered wooden bench standing green bunch green field person holding holding baseball painted birds standing crowd people people collage brick brown forest ocean flying water group birds large group aerial laying scissors statue riding flock birds table filled sitting chair penguin holding elephant picture grassy small street hillside floating water sitting couch graffiti covered sailboat middle middle water umbrella around stage sandy beach bunch flowers stuffed animal wooden floor holding tennis tennis racquet water sailboat large water board figurine concrete soccer dress orange sword broken table topped stairs leading picture taken large plate dressed series stick perched bathing fruit flying kites different types piece paper close picture sailboat floating green hillside forest filled snowboard background women shoes items floor couch baseball house surface object blurry walking bikini clock sunset bunch rocks filled trees upside number three different distance toilet train carriage grass plant blanket river wildflowers bouquet squirrel place couple photograph rocks plane children empty towel covered slope snowy landscape playing soccer train track fashioned picture bottle alcohol colorful umbrellas umbrellas hanging taken mountain jumping giraffe riding desert string kites cloudy collection shining paper adult living medieval pottery outfit horseback teddy showing porcelain frisbee sheet sticking hands woods candles boats glass"
arvore_ontologica = popular_arvore_antologica(word_cloud_data)

def print_tree(arvore, indent=0):
    print(' ' * indent + arvore.nome)
    for no in arvore.nos:
        print_tree(no, indent + 4)

print_tree(arvore_ontologica)

def get_long_words(phrases):
    long_words = []

    for phrase in phrases:
        words = phrase.split()
        for word in words:
            if len(word) > 4:
                long_words.append(word)

    return ' '.join(long_words)

def remove_duplicates(string):
    words = string.split()
    unique_words = set(words)
    return ' '.join(unique_words)

def validate_words(preds):
  words = get_long_words(preds)
  words = remove_duplicates(words)
  words_array = words.split()
  array_size = len(words_array)
  verify_array = []
  for word in words_array:
      verify_array.append(arvore_ontologica.busca_palavra(word))
  percentage = (verify_array.count(True)/array_size)*100
  print(percentage)

import glob, os

seed = torch.manual_seed(0)
dirbase="/content/drive/MyDrive/IA-Semestre1/Projeto-IA/train"
folder = dirbase
Text_to_Speech_Setup(engine)

for style in sorted(glob.glob("*")):
      path = folder+"/"+style
      orig_img = Image.open(path)
      imageset = Image_Cropping(orig_img)
      plt.imshow(orig_img)
      plt.axis("off")
      plt.show()
      preds = ImageCaptioning(orig_img)
      print(preds)
      #validate_words(preds)
      #Run_Text_to_Speech(preds)